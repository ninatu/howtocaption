arch:
  type: BlipVTDecoderModel
  args:
    vit: 'base'
    init_from_pretrained: 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'
    med_config: configs/med_config.json
    tie_encoder_decoder_weights: false

    train_contrastive: false
    train_captioning: true
    train_itm: false

    train_max_text_length: 32
    queue_size: 2048

train_data_loader:
  - type: VideoDataLoader
    args:
      dataset_type: YouCook2
      dataset_args:
        data_root: data/youcook
        num_frames: 16
        max_text_length: 32
      num_workers: 16
      batch_size: 32
      split: train
      transform: 'train'

valid_data_loader:
- type: VideoDataLoader
  args:
    dataset_type: YouCook2
    dataset_args:
      data_root: data/youcook
      num_frames: 16
      max_text_length: 32
    num_workers: 16
    batch_size: 32
    split: val
    transform: 'test'


optimizer:
  type: AdamW
  args:
    lr: 1.0e-05
    weight_decay: 0.05

lr_scheduler:
  type: CosineAnnealingLR
  args:
    T_max: 10
    eta_min: 0

save_dir: output

trainer:
  type: VL_Trainer
  args:
    resume_only_model: True
    load_strict: False

    lr_scheduler_update: 'iter'
    init_retrieval: false
    init_nlp: false

    epochs: 10
    save_latest: True
    save_period: 1000000
    monitor: 'off'
    mixed_precision: true

    log_visual_input_at_start: True
    freq_visual_input: 100000
    nlp_freq_eval: 1
    freq_eval: 100000
    retrieval_freq_eval: 100000

    eval_args:
      num_beams: 1
      min_length: 0
      max_length: 20
      top_p: 1.0
      repetition_penalty: 1.0

    clip_grad: 20
